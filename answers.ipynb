{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Bernoulli Mixture Model: Theory\n",
    "\n",
    "To train a Bernoulli Mixture Model, the formulae are:\n",
    "\n",
    "- Expectation step\n",
    "\n",
    "$$z_{n, k} \\leftarrow \\frac{\\pi_k \\prod_{i = 1}^D \\mu_{k, i}^{x_{n, i}} (1 - \\mu_{k, i})^{1 - x_{n, i}} }{\\sum_{m = 1}^K \\pi_m \\prod_{i = 1}^D \\mu_{m, i}^{x_{n, i}} (1 - \\mu_{m, i})^{1 - x_{n, i}}}$$\n",
    "\n",
    "- Maximization step\n",
    "\n",
    "$$\\mathbf{\\mu_m} \\leftarrow \\mathbf{\\bar{x}_m}$$\n",
    "\n",
    "$$\\pi_m \\leftarrow \\frac{N_m}{N}$$\n",
    "\n",
    "where $\\mathbf{\\bar{x}_m} = \\frac{1}{N_m} \\sum_{n = 1}^N z_{n, m} \\mathbf{x_n}$ and $N_m = \\sum_{n = 1}^N z_{n, m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: BMM Implementation\n",
    "\n",
    "**(1)** see `bmm.py` for the complete implementation of the BMM\n",
    "\n",
    "the source code of this project is available at https://github.com/toogy/mnist-em-bmm-gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "\n",
    "data_path = '/home/data/ml/mnist'\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we load pre-calculated k-means\n",
    "\n",
    "import kmeans as kmeans_\n",
    "\n",
    "kmeans = kmeans_.load_kmeans('kmeans-20.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "import bmm\n",
    "import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading the data\n",
    "from mnist import load_mnist\n",
    "\n",
    "train_data, train_labels = load_mnist(dataset='training', path=data_path)\n",
    "\n",
    "# pre-processing the data (reshape + making it binary)\n",
    "\n",
    "train_data = np.reshape(train_data, (60000, 784))\n",
    "train_data_binary = np.where(train_data > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating our model\n",
    "model = bmm.bmm(k, n_iter=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using random heuristic to initialize the means\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Plot of the means $\\mathbf{\\mu}$ of the learnt mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize.plot_means(model.means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** It is not possible to have one center per class with only 10 components even though there are only 10 different digits. Multiple components can represent the same digits (as we can see from the plot). When this happens, it is not possible to represent all of them with only 10 components.\n",
    "\n",
    "It is possible to avoid this by initializing each component's $\\mu_k$ to the mean of the corresponding digit calculated from the labelized dataset. But then it becomes **supervised** learning, which is not what we want.\n",
    "\n",
    "Here is the result with this kind of initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = bmm.bmm(10, verbose=True)\n",
    "model.fit(train_data_binary, means_init_heuristic='data_classes_mean', labels=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize.plot_means(model.means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)** For each label we select the subset of the data that corresponds to this label and train a `bmm` to represent the corresponding class. We then have 10 `bmm` which, together, form a digit classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import classifier\n",
    "\n",
    "# number of components for each BMM\n",
    "k = 7\n",
    "\n",
    "bayesian_classifier = classifier.classifier(k, means_init_heuristic='kmeans',\n",
    "                                            means=kmeans, model_type='bmm')\n",
    "\n",
    "bayesian_classifier.fit(train_data_binary, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize.plot_means(bayesian_classifier.models[3].means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize.plot_means(bayesian_classifier.models[8].means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data, test_labels = load_mnist(dataset='testing', path=data_path)\n",
    "test_data = np.reshape(test_data, (test_data.shape[0], 784))\n",
    "test_data_binary = np.where(test_data > 0.5, 1, 0)\n",
    "\n",
    "label_set = set(train_labels)\n",
    "\n",
    "predicted_labels = bayesian_classifier.predict(test_data_binary, label_set)\n",
    "\n",
    "print('accuracy: {}'.format(np.mean(predicted_labels == test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Gaussian Mixture Models\n",
    "\n",
    "BMM are adapted to binary images because they work with 0s and 1s. MNIST data initially was in the range $[0, 255]$. By binarizing the images, information is lost when it could make the model more accurate. GMM can work with real numbers and perform better than BMM for classifying digits.\n",
    "\n",
    "The Gaussian mixture distribution can be written as a linear superposition of Gaussians in the form\n",
    "\n",
    "$$p(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "d = 40\n",
    "\n",
    "reducer = sklearn.decomposition.PCA(n_components=d)\n",
    "reducer.fit(train_data)\n",
    "\n",
    "train_data_reduced = reducer.transform(train_data)\n",
    "test_data_reduced = reducer.transform(test_data)\n",
    "kmeans_reduced = reducer.transform(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gmm\n",
    "\n",
    "k = 5\n",
    "\n",
    "model = gmm.gmm(k, n_iter=20, verbose=True)\n",
    "model.fit(train_data_reduced, means_init_heuristic='kmeans', means=kmeans_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means_projected = reducer.inverse_transform(model.means)\n",
    "visualize.plot_means(means_projected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bayesian_classifier = classifier.classifier(k, model_type='gmm',\n",
    "                                            means_init_heuristic='kmeans',\n",
    "                                            means=kmeans_reduced)\n",
    "bayesian_classifier.fit(train_data_reduced, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means_projected = reducer.inverse_transform(bayesian_classifier.models[4].means)\n",
    "visualize.plot_means(means_projected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_labels = bayesian_classifier.predict(test_data_reduced, label_set)\n",
    "\n",
    "print('accuracy: {}'.format(np.mean(predicted_labels == test_labels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
